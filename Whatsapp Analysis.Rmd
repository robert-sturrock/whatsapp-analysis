---
title: "R Whatsapp Analysis"
output: html_notebook
---

This code sets out how to upload and then analyse Whatsapp text data using both charts and networks.

```{r}

library("RCurl")
library("data.table")
library(stringr)

#tab = read.delim(getURL("https://raw.githubusercontent.com/robert-sturrock/whatsapp-analysis/master/chat.txt"))
#write.table(tab, file="text.csv",sep=",",col.names=FALSE,row.names=FALSE)


tab = read.delim("C:/Users/rober/Desktop/chat.txt")
write.table(tab, file="C:/Users/rober/Desktop/text.csv",sep=",",col.names=FALSE,row.names=FALSE)

chat <- fread("C:/Users/rober/Desktop/text.csv", blank.lines.skip = TRUE)
names(chat) <- "string"

```

Now we need to look at the data and format it approperiately 

```{r}
head(chat)

```

So we can see we have a date, time, sender, and message. 

The first obvious issue here is that one of the senders names has been messed up. We can fix this using the regex commands in R 

```{r}

chat$string <- gsub("â.ª.*: ", "Debbie Blair: ", chat$string )
head(chat$string)




```

Now that we have made that correction we need to split the string of text into the individual components identified above: 

```{r}
library(tidyverse)

regx <- "(\\d+/\\d+/\\d+),\\s+(\\d+:\\d+:\\d+ [A|P]M):\\s+(\\w+\\s*\\w+):\\s+(.*)"

tmp <- chat %>% 
    extract(string, c("date", "time", "sender", "message"), regx, remove=FALSE)


#eliminate NA rows (for now)
tmp <- tmp[complete.cases(tmp)]

```

We now have our desired variables. Lets build some basic functionality like search

```{r}

#regular expression search
grep("sex", tmp$message, value=TRUE)


#function that returns other columns of the data as well
text_search <- function(text, df){
  tmp <- df
  tmp_index <- grep(text, tmp$message)
  tmp[c(tmp_index),c("sender", "message")]
}

text_search("sex", tmp)

#function to search around a specific instance (message before and after)

# this function generates another function that allows the user to select which instance of a word, and the 
# number of lines around that word, they want to see. 

conv_search <- function(text, df){ 
  tmp <- df
  tmp_index <- grep(text, tmp$message)
  
  #return lines
  index <- tmp_index
  
  function(instance, num_lines){
    if (num_lines %% 2 !=0){"error, needs even number"} else {
    
    instance <- index[instance]
    start <- instance - num_lines/2
    end   <- instance + num_lines/2  
    tmp[start:end, c("sender", "message")]
    } 
  }
  
  
}

sex_search <- conv_search("sex", tmp)
sex_search(2,6)

library(wordcloud)


```



Great. So we built some functions that make it easy to search text. 

But searching isn't really the best way to get data about what people talked about. Charts on the other hand are awesome. 

I start by building a simple frequency chart of how often a given work was said


```{r}

# Create a single block of text with all the words

test <- unlist(tmp$message[tmp$sender == "Liam Kirwin"])
test <- unlist(tmp$message)
test <- paste(test, sep="",collapse =" ")

# load required packages

library(tm)
library(dplyr)
library(xtable)

docs <- Corpus(VectorSource(test)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(tolower)  %>%
  tm_map(removeWords, stopwords("english")) %>%
  tm_map(stripWhitespace) %>%
  tm_map(PlainTextDocument)

tdm <- TermDocumentMatrix(docs) %>%
  as.matrix()

tdm <- as.matrix(tdm[,1])
tdm <- as.matrix(tdm[order(tdm, decreasing = TRUE),])
#tdm <- as.matrix(tdm[10:17546,])
#tdm <- tdm[order(tdm, decreasing = TRUE),]



wordcloud(row.names(tdm), tdm, min.freq = 35, scale=c(5, .2), random.order = FALSE, random.color = FALSE, colors= c("indianred1","indianred2","indianred3","indianred"))




#Now we create a wordcloud function 

WA_wordcloud <- function(sender, df, min.freq){
  
  #required packages
  #require(tm)
  #require(dplyr)
  #require(xtable)
  #require(wordcloud)
  #require(stringr)
  
  #set function
  tmp <- unlist(df$message[df$sender == sender])
  tmp <- paste(tmp, sep="",collapse ="")
  
  #eliminate all non alpha numberic
  tmp <- str_replace_all(tmp, "[^a-zA-Z0-9]"," ")

  
  #create term document matrix and format it
  tmp_docs <- Corpus(VectorSource(tmp)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(tolower)  %>%
  tm_map(removeWords, stopwords("english")) %>%
  tm_map(removeWords,"image") %>%
  tm_map(stripWhitespace) %>%
  tm_map(PlainTextDocument)
  
  tmp_tdm <- TermDocumentMatrix(tmp_docs) %>%
  as.matrix()
  
  tmp_tdm <- as.matrix(tmp_tdm[,1])
  tmp_tdm <- as.matrix(tmp_tdm[order(tmp_tdm, decreasing = TRUE),])
  
  #create wordcloud
  layout(matrix(c(1,2), nrow = 2), heights = c(1,2))
  par(mar = rep(0,4))
  plot.new()
  text(x=0.5, y=0.5, sender)
  
  
  wordcloud(row.names(tmp_tdm), tmp_tdm, 
            min.freq = min.freq, 
            scale=c(2, .1), 
            random.order = FALSE, 
            random.color = FALSE,  
            colors= c("indianred1","indianred2","indianred3","indianred"))


  
}


WA_wordcloud(sender = "Liam Kirwin", df = tmp, min.freq = 15)

flat_names <- list("Liam Kirwin", "Isabel Lachenauer", "Robert", "Debbie Blair", "Elizabeth Stone")

lapply(flat_names, function(x) WA_wordcloud(x, df = tmp, min.freq = 25))

```


Another potentially interesting idea is to do a wordcloud of words that people use more than the average. This would involve calculating two data.frames/matricies with words and frequencies. And then subtracting the individual words from the average. 


Step 1: 
```{r}

#create a function that creates a tdm 

tdm_creator <- function(sender="all", df){
  
  #set function
  if(sender != "all"){tmp <- unlist(df$message[df$sender == sender])} else {tmp <- unlist(df$message)}
  tmp <- paste(tmp, sep="",collapse ="")
  
  #eliminate all non alpha numberic
  tmp <- str_replace_all(tmp, "[^a-zA-Z0-9]"," ")

  
  #create term document matrix and format it
  tmp_docs <- Corpus(VectorSource(tmp)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(tolower)  %>%
  tm_map(removeWords, stopwords("english")) %>%
  tm_map(removeWords,"image") %>%
  tm_map(stripWhitespace) %>%
  tm_map(PlainTextDocument)
  
  tmp_tdm <- TermDocumentMatrix(tmp_docs) %>%
  as.matrix()
  
  tmp_tdm <- as.matrix(tmp_tdm[,1])
  as.matrix(tmp_tdm[order(tmp_tdm, decreasing = TRUE),])
}



word_df <- function(matr){
  data.frame(word = row.names(matr), freq = matr)
}

#create data.frames to compare
isabel <- tdm_creator(sender = "Isabel Lachenauer", df = tmp)
liam <- tdm_creator(sender = "Liam Kirwin", df = tmp)
all <- tdm_creator(df = tmp)

isabel <- word_df(isabel)
liam <- word_df(liam)
all <- word_df(all)

t <- left_join(liam, all, by = "word")
t <- left_join(isabel, all, by = "word")

#identify words that liam says relatively more than others

t$diff <- t$freq.x - t$freq.y/5


t %>% arrange(desc(diff)) %>% head()

#control also for frequency that the word appears in the general group chat 

t <- t[t$freq.x <= t$freq.y,]
t$diff2 <- (t$freq.x - t$freq.y/5)/t$freq.y


t %>% arrange(desc(diff2)) %>% head()

#create a word cloud of most unique words

t2 <- t %>% filter(diff>0)

wordcloud(t2$word, t2$diff,
            scale=c(3, .1), 
            random.order = FALSE, 
            random.color = FALSE,  
            colors= c("indianred1","indianred2","indianred3","indianred"))

# use alternative method, but only use words that appear at least 3 times
t3 <- t %>% filter(diff2>0 & freq.x >3)


wordcloud(t3$word, t3$diff2,
            scale=c(3, .1), 
            max.word = 100,
            random.order = FALSE, 
            random.color = FALSE,  
            colors= c("indianred1","indianred2","indianred3","indianred"))

```



```{r}

# Function that creates a unique word cloud by person 

unique_wordcloud <- function(df, sender, type){
  
  #total senders
  tot_senders <- length(unique(df$sender))
  
  #create tdm
  person <- word_df(tdm_creator(sender = sender, df = tmp))
  all <- word_df(tdm_creator(df = tmp))
  
  t <- left_join(person, all, by = "word")

  #identify words that liam says relatively more than others
  
  t$diff <- t$freq.x - t$freq.y/tot_senders
  
  #control also for frequency that the word appears in the general group chat 
  
  t <- t[t$freq.x <= t$freq.y,]
  t$diff2 <- (t$freq.x - t$freq.y/tot_senders)/t$freq.y
  
  #create a word cloud of most unique words
  
  t2 <- t %>% filter(diff>0)
  t3 <- t %>% filter(diff2>0 & freq.x >3)

  
  
  if (type == 1){
  wordcloud(t2$word, t2$diff,
              scale=c(3, .1), 
              random.order = FALSE, 
              random.color = FALSE,  
              colors= c("indianred1","indianred2","indianred3","indianred"))} 
  
  else if (type ==2){
  wordcloud(t3$word, t3$diff2,
            scale=c(3, .1), 
            random.order = FALSE, 
            random.color = FALSE,  
            colors= c("indianred1","indianred2","indianred3","indianred"))
  } else {stop('invalid type selected')}
  
    
  
}

unique_wordcloud(df = tmp, sender = "Elizabeth Stone", type = 2)
unique_wordcloud(df = tmp, sender = "Robert", type = 2)






flat_names <- list("Liam Kirwin", "Isabel Lachenauer", "Robert", "Debbie Blair", "Elizabeth Stone")

lapply(flat_names, function(x) unique_wordcloud(sender = x, df = tmp, type = 2))


```

# Words said over time

So now I need to replicate some of the work I did earlier, namely plotting the frequency of words over time


```{r}


#step 1:

#count the number of times a word appears by date
tmp %>% group_by(date) %>% summarise(number = n())






```


#who talks to who the most


```{r}



#write a function that looks at most frequent reponders


response_summary <- function(df, person, type){

library(scales)
#select sender 
  tmp <- df 
  tmp$indicator <- 0 
  tmp[tmp$sender == person,]$indicator <- 1 
  tmp$ind <- lag(tmp$indicator)

  #calculate total messages by sender 
  tmp <- tmp %>% group_by(sender) %>%
  mutate(total_messages = n()) 
  tmp
  
  #create statistics
  if (type == 1){ tmp %>% group_by(sender) %>% 
      filter(ind == 1) %>% 
      summarise(total_responses = n())
  
  } else if (type == 2){ tmp %>% group_by(sender) %>% 
      filter(ind == 1) %>% 
      mutate(total_responses_to_sender = n()) %>% 
      mutate(done = total_responses_to_sender/total_messages) %>% 
      summarise("% of all messages made in response to sender" = percent(mean(done)))} }


response_summary(tmp, "Liam Kirwin", type = 2)


# Create a response matrix


response_matrix <- function(df, type){

senders <- response_summary(tmp, flat_names[[1]], type = type)$sender


responses_total <- cbind(as.data.frame(lapply(senders, function(x)
response_summary(df, x, type = type)[[2]]))) 
names(responses_total) <- senders 
row.names(responses_total) <- senders 
responses_total


}

response_matrix(tmp, 1)

#check how equally people respond to each other

mat1 <- as.matrix(response_matrix(tmp, 1)) # how often x responded to y 
mat2 <- t(mat1) # how often y responded to x

#so what this means is that mat1[1,2] - mat2[1,2] is how often Lizzie replied to  Debbie - how often Debbie replied to Lizzie

mat1 
mat2

# the results show that people in our conversation are quite equal. Which is what we'd expect in conversations where two people are most important/no one person is avoiding the other.

(mat1 - mat2)


# total messages people sent

lapply(response_matrix(tmp,1), sum)

# do this by word count 
```





# Duplicate the functions above, but apply it to individual words


```{r}



#write a function that looks at most frequent reponders


response_summary_word <- function(df, person, type, word){
browser()
library(scales)
library(magrittr)
#select sender 
  tmp <- df 
  tmp$indicator <- 0 
  if (length(tmp[tmp$sender == person & grepl(word, tmp$message),]$indicator) > 0){
    tmp[tmp$sender == person & grepl(word, tmp$message),]$indicator <- 1 
  } else { 
    tmp$indicator <- 0
  }
  tmp$indicator <- lag(tmp$indicator)
  tmp %<>% mutate(indicator = ifelse(is.na(indicator), 0, indicator))

#calculate total messages by sender 
  tmp <- tmp %>% group_by(sender) %>%
    mutate(total_messages = n()) 
  

if (type == 1){ tmp %>% group_by(sender) %>% summarise(total_responses = sum(na.omit(indicator)))

} else if (type == 2){tmp %>% group_by(sender) %>%
    mutate(total_responses_to_sender = sum(na.omit(indicator))) %>%
    mutate(done = total_responses_to_sender/total_messages) %>% 
    summarise("% of all messages made in response to sender" = percent(mean(done)))} }



response_summary_word(tmp, "Robert", type = 1, word="sex")






# Create a response matrix


response_matrix_word <- function(df, type, word){
senders <- response_summary(tmp, flat_names[[1]], type = 1)$sender


responses_total <- cbind(as.data.frame(lapply(senders, function(x)
response_summary_word(df, x, type = type, word = word)[,2]))) 
names(responses_total) <- senders 
row.names(responses_total) <- senders 
responses_total


}

response_matrix_word(tmp, 1, word = "lol")


lapply(senders, function(x){response_summary_word(tmp,x,type=1,word="sex")})

response_summary_word(tmp, "Robert", type = 1, word="sex")




```









```{r}


#check how equally people respond to each other

mat1 <- as.matrix(response_matrix_word(tmp, 1, "like")) # how often x responded to y 
mat2 <- t(mat1) # how often y responded to x


(mat1 - mat2)


# total messages people sent

lapply(response_matrix(tmp,1), sum)

# do this by word count 
```





# Next: write some of the functions using SE

```{r}


#select groups usinge SE

freq_plot <- function(df, word, v1, v2){

tmp <- df tmp %>% mutate(date = myd(date)) %>%
  group_by_(v1, v2) %>%
  filter(grepl(word, message)) %>%  
  summarise(count = n()) %>%
  ggplot(aes(x = date, y = count, color = sender)) + geom_point() + ylim(0,NA) + theme_bw() }


freq_plot(tmp, "sex", v1 = "sender", v2 = "date")

#summary table of two variable

tw_tab <- function(df, v1, v2){

tmp <- df

tmp %>% group_by_(v1, v2) %>% 
  mutate(tot=n()) %>% 
  ungroup() %>% 
  group_by_(v1) %>% 
  mutate(perc = tot/n()) %>% 
  ungroup() %>% group_by_(v1, v2) %>% 
  summarise(end = mean(perc)) %>% dcast(paste(v1, '~', v2), value.var = "end")

}

# plot share of conversation over time 
tw_tab(tmp, "date", "sender") %>%
  mutate(date = mdy(date)) %>% 
  melt(id.vars="date") %>% 
  ggplot(aes(x = date, y = value, color = variable, fill = variable)) + geom_col()




```








# Next: turn the response matrix into a network diagram

In order to do this I basically want to depict the relationships between people
and how important people are to the conversation.

Features include:

* nodes that are sized based on number of messages a person sent * directional
flows that are related to number of messages sent * do this for individual words


```{r}

library(visNetwork) 
library(igraph)


# Create nodes data.frame

library(dplyr) 
library(magrittr)

# create names
nodes <- data.frame(select(tmp, sender) %>% distinct()) 
names(nodes) <- "id"


# create number of messages sent 
totals <- select(tmp, sender) %>%
  group_by(sender) %>% 
  summarise(sum = n()) 

nodes$size <- (totals$sum *100)/sum(totals$sum)
# Create edges of data.frame 

responses <- response_matrix(tmp, 1) 

responses <-as.data.frame(responses)
responses$name <- row.names(responses) 
responses_m <- melt(responses) 
names(responses_m) <- c("from","to", "number")

# format as network

edges <- responses_m 
edges$to <- as.character(edges$to) 
edges$width <- (edges$number * 100)/sum(totals$sum)
edges$title <- paste("<p style=\"color:black\">",edges$number,"</p>")

# add in node colors, names 
nodes$group <- nodes$id 
nodes$label <- nodes$id 
nodes$title <- paste("<p style=\"color: black\">",totals$sum,"</p>")

# plot library(visNetwork) 
visNetwork(nodes, edges, width = "100%") %>%
visPhysics( solver ='forceAtlas2Based')




```










# New additions:

* do analysis of messages by number of words (done)
* do network and table for an individual word (table done)
* topic analysis 
* build app that allows you to upload file to analyse (produces network and word clouds)


# Count number of words

```{r}

# Create variable that is number of words per message

test <- "this message has many words in it"
length(unlist(strsplit(test, split = " ")))

test2 <- data.frame(c = c("message with three", "message with two"), stringsAsFactors = FALSE)
str(test2)
length(unlist(strsplit(test2$c, split = " ")))


library(stringr)

# create variable with number of words in each line

tmp %<>% ungroup
tmp %>% mutate(number_words = str_count(message, " ")) %>% group_by(sender) %>% summarise(total_words = sum(number_words), avg_message_length = total_words/mean(total_messages))



```


# Word sepecific matrix


```{r}

library(visNetwork) 
library(igraph)


# Create nodes data.frame

library(dplyr) 
library(magrittr)

# create names
nodes <- data.frame(select(tmp, sender) %>% distinct()) 
names(nodes) <- "id"


# create number of messages sent 
totals <- select(tmp, sender, message) %>%
  grep()
  group_by(sender) %>% 
  summarise(sum = n()) 

nodes$size <- (totals$sum *100)/sum(totals$sum)
# Create edges of data.frame 

responses <- response_matrix_word(tmp, 1, word = "sex") 

responses <-as.data.frame(responses)
responses$name <- row.names(responses) 
responses_m <- melt(responses) 
names(responses_m) <- c("from","to", "number")

# format as network

edges <- responses_m 
edges$to <- as.character(edges$to) 
edges$width <- (edges$number * 50)/sum(edges$number)
edges$title <- paste("<p style=\"color:black\">",edges$number,"</p>")
edges <- edges[edges$number > 0,]

# add in node colors, names 
nodes$group <- nodes$id 
nodes$label <- nodes$id 
nodes$title <- paste("<p style=\"color: black\">",totals$sum,"</p>")

# plot library(visNetwork) 
visNetwork(nodes, edges, width = "100%") %>%
visPhysics( solver ='forceAtlas2Based')




```




# Create shiny app 

The first stage of creating this shiny app is just making it so that the user can select (via an input box) what word the would like to look at the network of. 

Ideally I would then also make it so the can just look at the matrix of all messages sent (or perhaps have this as a seperate part of the app for comparison purposes).

Later stages of the app should then add some additional summary measures (person specific word cloud, messages sent over time, messages sent over time containing a particular word, etc). 


```{r}


require(shiny)
require(visNetwork)

server <- function(input, output) {
  output$network <- renderVisNetwork({
 
    visNetwork(nodes, edges)
  })
}

ui <- fluidPage(
    visNetworkOutput("network")
)

shinyApp(ui = ui, server = server)



```





