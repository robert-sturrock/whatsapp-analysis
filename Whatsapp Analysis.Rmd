---
title: "R Whatsapp Analysis"
output: html_notebook
---

This code sets out how to upload and then analyse Whatsapp text data using both charts and networks.

```{r}

library("RCurl")
library("data.table")
library(stringr)

#tab = read.delim(getURL("https://raw.githubusercontent.com/robert-sturrock/whatsapp-analysis/master/chat.txt"))
#write.table(tab, file="text.csv",sep=",",col.names=FALSE,row.names=FALSE)


tab = read.delim("C:/Users/rober/Desktop/chat.txt")
write.table(tab, file="C:/Users/rober/Desktop/text.csv",sep=",",col.names=FALSE,row.names=FALSE)

chat <- fread("C:/Users/rober/Desktop/text.csv", blank.lines.skip = TRUE)
names(chat) <- "string"

```

Now we need to look at the data and format it approperiately 

```{r}
head(chat)

```

So we can see we have a date, time, sender, and message. 

The first obvious issue here is that one of the senders names has been messed up. We can fix this using the regex commands in R 

```{r}

chat$string <- gsub("â.ª.*: ", "Debbie Blair: ", chat$string )
head(chat$string)




```

Now that we have made that correction we need to split the string of text into the individual components identified above: 

```{r}
library(tidyverse)

regx <- "(\\d+/\\d+/\\d+),\\s+(\\d+:\\d+:\\d+ [A|P]M):\\s+(\\w+\\s*\\w+):\\s+(.*)"

tmp <- chat %>% 
    extract(string, c("date", "time", "sender", "message"), regx, remove=FALSE)


#eliminate NA rows (for now)
tmp <- tmp[complete.cases(tmp)]

```

We now have our desired variables. Lets build some basic functionality like search

```{r}

#regular expression search
grep("sex", tmp$message, value=TRUE)


#function that returns other columns of the data as well
text_search <- function(text, df){
  tmp <- df
  tmp_index <- grep(text, tmp$message)
  tmp[c(tmp_index),c("sender", "message")]
}

text_search("sex", tmp)

#function to search around a specific instance (message before and after)


conv_search <- function(text, df){ 
  tmp <- df
  tmp_index <- grep(text, tmp$message)
  
  #return lines
  index <- tmp_index
  
  function(instance, num_lines){
    if (num_lines %% 2 !=0){"error, needs even number"} else {
    
    instance <- index[instance]
    start <- instance - num_lines/2
    end   <- instance + num_lines/2  
    tmp[start:end, c("sender", "message")]
    } 
  }
  
  
}

sex_search <- conv_search("sex", tmp)
sex_search(2,6)

library(wordcloud)


```



Great. So we built some functions that make it easy to search text. 

But searching isn't really the best way to get data about what people talked about. Charts on the other hand are awesome. 

I start by building a simple frequency chart of how often a given work was said


```{r}

# Create a single block of text with all the words

test <- unlist(tmp$message[tmp$sender == "Liam Kirwin"])
test <- unlist(tmp$message)
test <- paste(test, sep="",collapse =" ")

# load required packages

library(tm)
library(dplyr)
library(xtable)

docs <- Corpus(VectorSource(test)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(tolower)  %>%
  tm_map(removeWords, stopwords("english")) %>%
  tm_map(stripWhitespace) %>%
  tm_map(PlainTextDocument)

tdm <- TermDocumentMatrix(docs) %>%
  as.matrix()

tdm <- as.matrix(tdm[,1])
tdm <- as.matrix(tdm[order(tdm, decreasing = TRUE),])
#tdm <- as.matrix(tdm[10:17546,])
#tdm <- tdm[order(tdm, decreasing = TRUE),]



wordcloud(row.names(tdm), tdm, min.freq = 35, scale=c(5, .2), random.order = FALSE, random.color = FALSE, colors= c("indianred1","indianred2","indianred3","indianred"))




#Now we create a wordcloud function 

WA_wordcloud <- function(sender, df, min.freq){
  
  #required packages
  #require(tm)
  #require(dplyr)
  #require(xtable)
  #require(wordcloud)
  #require(stringr)
  
  #set function
  tmp <- unlist(df$message[df$sender == sender])
  tmp <- paste(tmp, sep="",collapse ="")
  
  #eliminate all non alpha numberic
  tmp <- str_replace_all(tmp, "[^a-zA-Z0-9]"," ")

  
  #create term document matrix and format it
  tmp_docs <- Corpus(VectorSource(tmp)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(tolower)  %>%
  tm_map(removeWords, stopwords("english")) %>%
  tm_map(removeWords,"image") %>%
  tm_map(stripWhitespace) %>%
  tm_map(PlainTextDocument)
  
  tmp_tdm <- TermDocumentMatrix(tmp_docs) %>%
  as.matrix()
  
  tmp_tdm <- as.matrix(tmp_tdm[,1])
  tmp_tdm <- as.matrix(tmp_tdm[order(tmp_tdm, decreasing = TRUE),])
  
  #create wordcloud
  layout(matrix(c(1,2), nrow = 2), heights = c(1,2))
  par(mar = rep(0,4))
  plot.new()
  text(x=0.5, y=0.5, sender)
  
  
  wordcloud(row.names(tmp_tdm), tmp_tdm, 
            min.freq = min.freq, 
            scale=c(2, .1), 
            random.order = FALSE, 
            random.color = FALSE,  
            colors= c("indianred1","indianred2","indianred3","indianred"))


  
}


WA_wordcloud(sender = "Liam Kirwin", df = tmp, min.freq = 15)

flat_names <- list("Liam Kirwin", "Isabel Lachenauer", "Robert", "Debbie Blair", "Elizabeth Stone")

lapply(flat_names, function(x) WA_wordcloud(x, df = tmp, min.freq = 25))

```


Another potentially interesting idea is to do a wordcloud of words that people use more than the average. This would involve calculating two data.frames/matricies with words and frequencies. And then subtracting the individual words from the average. 

```{r}

#create a function that creates a tdm 

tdm_creator <- function(sender="all", df){
  
  #set function
  if(sender != "all"){tmp <- unlist(df$message[df$sender == sender])} else {tmp <- unlist(df$message)}
  tmp <- paste(tmp, sep="",collapse ="")
  
  #eliminate all non alpha numberic
  tmp <- str_replace_all(tmp, "[^a-zA-Z0-9]"," ")

  
  #create term document matrix and format it
  tmp_docs <- Corpus(VectorSource(tmp)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(tolower)  %>%
  tm_map(removeWords, stopwords("english")) %>%
  tm_map(removeWords,"image") %>%
  tm_map(stripWhitespace) %>%
  tm_map(PlainTextDocument)
  
  tmp_tdm <- TermDocumentMatrix(tmp_docs) %>%
  as.matrix()
  
  tmp_tdm <- as.matrix(tmp_tdm[,1])
  as.matrix(tmp_tdm[order(tmp_tdm, decreasing = TRUE),])
}



word_df <- function(matr){
  data.frame(word = row.names(matr), freq = matr)
}

#create data.frames to compare
isabel <- tdm_creator(sender = "Isabel Lachenauer", df = tmp)
liam <- tdm_creator(sender = "Liam Kirwin", df = tmp)
all <- tdm_creator(df = tmp)

isabel <- word_df(isabel)
liam <- word_df(liam)
all <- word_df(all)

t <- left_join(liam, all, by = "word")
t <- left_join(isabel, all, by = "word")

#identify words that liam says relatively more than others

t$diff <- t$freq.x - t$freq.y/5


t %>% arrange(desc(diff)) %>% head()

#control also for frequency that the word appears in the general group chat 

t <- t[t$freq.x <= t$freq.y,]
t$diff2 <- (t$freq.x - t$freq.y/5)/t$freq.y


t %>% arrange(desc(diff2)) %>% head()

#create a word cloud of most unique words

t2 <- t %>% filter(diff>0)

wordcloud(t2$word, t2$diff,
            scale=c(3, .1), 
            random.order = FALSE, 
            random.color = FALSE,  
            colors= c("indianred1","indianred2","indianred3","indianred"))

# use alternative method, but only use words that appear at least 3 times
t3 <- t %>% filter(diff2>0 & freq.x >3)


wordcloud(t3$word, t3$diff2,
            scale=c(3, .1), 
            max.word = 100,
            random.order = FALSE, 
            random.color = FALSE,  
            colors= c("indianred1","indianred2","indianred3","indianred"))

```



```{r}

# Function that creates a unique word cloud by person 

unique_wordcloud <- function(df, sender, type){
  
  #total senders
  tot_senders <- length(unique(df$sender))
  
  #create tdm
  person <- word_df(tdm_creator(sender = sender, df = tmp))
  all <- word_df(tdm_creator(df = tmp))
  
  t <- left_join(person, all, by = "word")

  #identify words that liam says relatively more than others
  
  t$diff <- t$freq.x - t$freq.y/tot_senders
  
  #control also for frequency that the word appears in the general group chat 
  
  t <- t[t$freq.x <= t$freq.y,]
  t$diff2 <- (t$freq.x - t$freq.y/tot_senders)/t$freq.y
  
  #create a word cloud of most unique words
  
  t2 <- t %>% filter(diff>0)
  t3 <- t %>% filter(diff2>0 & freq.x >3)

  
  
  if (type == 1){
  wordcloud(t2$word, t2$diff,
              scale=c(3, .1), 
              random.order = FALSE, 
              random.color = FALSE,  
              colors= c("indianred1","indianred2","indianred3","indianred"))} 
  
  else if (type ==2){
  wordcloud(t3$word, t3$diff2,
            scale=c(3, .1), 
            random.order = FALSE, 
            random.color = FALSE,  
            colors= c("indianred1","indianred2","indianred3","indianred"))
  } else {stop('invalid type selected')}
  
    
  
}

unique_wordcloud(df = tmp, sender = "Elizabeth Stone", type = 2)
unique_wordcloud(df = tmp, sender = "Robert", type = 2)






flat_names <- list("Liam Kirwin", "Isabel Lachenauer", "Robert", "Debbie Blair", "Elizabeth Stone")

lapply(flat_names, function(x) unique_wordcloud(sender = x, df = tmp, type = 2))


```

# Words said over time

So now I need to replicate some of the work I did earlier, namely plotting the frequency of words over time


```{r}


#step 1:

#count the number of times a word appears by date
tmp %>% group_by(date) %>% summarise(number = gre)






```

#who talks to who the most

#who talks to who the most


```{r}



#write a function that looks at most frequent reponders


response_summary <- function(df, sender, type){

library(scales)

#select sender 
  tmp <- df tmp$indicator <- 0 
  tmp[tmp$sender == sender,]$indicator <- 1 
  tmp$indicator <- lag(tmp$indicator)

  #calculate total messages by sender tmp <- tmp %>% group_by(sender) %>%
  mutate(total_messages = n()) tmp
  
  if (type == 1){ tmp %>% group_by(sender) %>% filter(indicator == 1) %>% 
  summarise(total_responses = n())
  
  } else if (type == 2){ tmp %>% group_by(sender) %>% filter(indicator == 1) %>% 
  mutate(total_responses_to_sender = n()) %>% mutate(done =
  total_responses_to_sender/total_messages) %>% summarise("% of all messages made
  in response to sender" = percent(mean(done)))} }


response_summary(tmp, "Liam Kirwin", type = 1)


# Create a response matrix


response_matrix <- function(df, type){

senders <- response_summary(tmp, flat_names[[1]], type = type)$sender


responses_total <- cbind(as.data.frame(lapply(senders, function(x)
response_summary(df, x, type = type)[[2]]))) names(responses_total) <- senders 
row.names(responses_total) <- senders responses_total


}

response_matrix(tmp, 1)

#check how equally people respond to each other

mat1 <- as.matrix(response_matrix(tmp, 1)) # how often x responded to y 
mat2 <- t(mat1) # how often y responded to x

#so what this means is that mat1[1,2] - mat2[1,2] is how often Lizzie replied to
Debbie - how often Debbie replied to Lizzie

mat1 
mat2

# the results show that people in our conversation are quite equal. Which is what we'd expect in conversations where two people are most important/no one person is avoiding the other.

(mat1 - mat2)


# total messages people sent

lapply(response_matrix(tmp,1), sum)

# do this by word count 
```





# Duplicate the functions above, but apply it to individual words


```{r}



#write a function that looks at most frequent reponders


response_summary_word <- function(df, sender, type, word){

library(scales)

#select sender 
  tmp <- df 
  tmp$indicator <- 0 
  tmp[tmp$sender == sender & grepl(word, tmp$message),]$indicator <- 1 
  tmp$indicator <- lag(tmp$indicator)

#calculate total messages by sender 
  tmp <- tmp %>% group_by(sender) %>%
    mutate(total_messages = n()) 
  tmp

if (type == 1){ tmp %>% group_by(sender) %>% filter(indicator == 1) %>% 
summarise(total_responses = n())

} else if (type == 2){ tmp %>% group_by(sender) %>% filter(indicator == 1) %>% 
mutate(total_responses_to_sender = n()) %>% mutate(done =
total_responses_to_sender/total_messages) %>% summarise("% of all messages made
in response to sender" = percent(mean(done)))} }


response_summary_word(tmp, "Isabel Lachenauer", type = 1, word="sad")


# Create a response matrix


response_matrix_word <- function(df, type, word){

senders <- response_summary(tmp, flat_names[[1]], type = type)$sender


responses_total <- cbind(as.data.frame(lapply(senders, function(x)
response_summary_word(df, x, type = type, word = word)[[2]]))) 
names(responses_total) <- senders row.names(responses_total) <- senders 
responses_total


}

response_matrix_word(tmp, 1, word = "sex")

#check how equally people respond to each other

mat1 <- as.matrix(response_matrix_word(tmp, 1, "like")) # how often x responded to y 
mat2 <- t(mat1) # how often y responded to x


(mat1 - mat2)


# total messages people sent

lapply(response_matrix(tmp,1), sum)

# do this by word count 
```










# Next: write some of the functions using SE

```{r}


#select groups usinge SE

freq_plot <- function(df, word, v1, v2){

tmp <- df tmp %>% mutate(date = myd(date)) %>%
  group_by_(v1, v2) %>%
  filter(grepl(word, message)) %>%  
  summarise(count = n()) %>%
  ggplot(aes(x = date, y = count, color = sender)) + geom_point() + ylim(0,NA) + theme_bw() }


freq_plot(tmp, "sex", v1 = "sender", v2 = "date")

#summary table of two variable

tw_tab <- function(df, v1, v2){

tmp <- df

tmp %>% group_by_(v1, v2) %>% 
  mutate(tot=n()) %>% 
  ungroup() %>% 
  group_by_(v1) %>% 
  mutate(perc = tot/n()) %>% 
  ungroup() %>% group_by_(v1, v2) %>% 
  summarise(end = mean(perc)) %>% dcast(paste(v1, '~', v2), value.var = "end")

}

# plot share of conversation over time 
tw_tab(tmp, "date", "sender") %>%
  mutate(date = mdy(date)) %>% 
  melt(id.vars="date") %>% 
  ggplot(aes(x = date, y = value, color = variable, fill = variable)) + geom_col()




```








# Next: turn the response matrix into a network diagram

In order to do this I basically want to depict the relationships between people
and how important people are to the conversation.

Features include:

* nodes that are sized based on number of messages a person sent * directional
flows that are related to number of messages sent * do this for individual words


```{r}

library(visNetwork) library(igraph)


# Create nodes data.frame

library(dplyr) library(magrittr)

# create names
nodes <- data.frame(select(tmp, sender) %>% distinct()) 
names(nodes) <- "id"

# create number of messages sent 
totals <- select(tmp, sender) %>%
  group_by(sender) %>% 
  summarise(sum = n()) 

nodes$size <- (totals$sum *100)/sum(totals$sum)

# Create edges of data.frame 

responses <- response_matrix(tmp, 1) 

responses <-nas.data.frame(responses)
responses$name <- row.names(responses) 
responses_m <- melt(responses) 
names(responses_m) <- c("from","to", "number")

# format as network

edges <- responses_m 
edges$to <- as.character(edges$to) 
edges$width <- (edges$number * 100)/sum(totals$sum)
edges$title <- paste("<p style=\"color:black\">",edges$number,"</p>")

# add in node colors, names nodes$group <- nodes$id nodes$label <- nodes$id 
nodes$title <- paste("<p style=\"color: black\">",totals$sum,"</p>")

# plot library(visNetwork) visNetwork(nodes, edges, width = "100%") %>%
visPhysics( solver ='forceAtlas2Based')




```



# New additions:

* do analysis of messages by number of words * do network and table for an
individual word * topic analysis * build app that allows you to upload file to
analyse (produces network and word clouds)


# Count number of words

```{r}

# Create variable that is number of words per message

test <- "this message has many words in it"
length(unlist(strsplit(test, split = " ")))

test2 <- data.frame(c = c("message with three", "message with two"), stringsAsFactors = FALSE)
str(test2)
length(unlist(strsplit(test2$c, split = " ")))


library(stringr)

# create variable with number of words in each line

tmp %<>% ungroup
tmp %>% mutate(number_words = str_count(message, " ")) %>% group_by(sender) %>% summarise(total_words = sum(number_words), avg_message_length = total_words/mean(total_messages))



```
